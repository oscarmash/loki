defaultRules:
  create: true
  rules:
    alertmanager: false
    etcd: false
    configReloaders: false
    general: false
    k8s: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: true
    kubeProxy: false
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: true
    kubeStateMetrics: false
    network: false
    node: true
    nodeExporterAlerting: false
    nodeExporterRecording: true
    prometheus: false
    prometheusOperator: false

additionalPrometheusRulesMap:

  ilimit.custom.watchdog:
    groups:
    - name: ilimit.watchdog
      rules:

      - alert: Watchdog
        annotations:
          description: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
          summary: An alert that should always be firing to certify that Alertmanager
            is working properly.
        expr: vector(1)
        labels:
          severity: none

  ilimit.custom.rules:
    groups:
    - name: ilimit.rules.pods
      rules:

      - alert: KubernetesPodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesContainerOomKiller
        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes container oom killer (instance {{ $labels.instance }})
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesPodNotHealthy
        expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes Pod not healthy (instance {{ $labels.instance }})
          description: "Pod has been in a non-ready state for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  ilimit.custom.etcd:
    groups:
    - name: ilimit.etcd
      rules:

      - alert: etcdDatabaseQuotaLowSpace
        expr: (last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m]))*100 > 95
        for: 10m
        labels:
          severity: warning
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
          summary: etcd cluster database is running full.          

      - alert: etcdExcessiveDatabaseGrowth
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": Predicting running out of disk space
            in the next four hours, based on write observations within the past four hours
            on etcd instance {{ $labels.instance }}, please check as it might be disruptive.'
          summary: etcd cluster database growing very fast.
        expr: |
          predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4*60*60) > etcd_server_quota_backend_bytes
        for: 10m
        labels:
          severity: warning

  ilimit.custom.replicas.mismatch:
    groups:
    - name: ilimit.replicas.mismatch
      rules:

      - alert: KubernetesDeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})
          description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesStatefulsetReplicasMismatch
        expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})
          description: "A StatefulSet does not match the expected number of replicas.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesDeploymentGenerationMismatch
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})
          description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


      - alert: KubernetesStatefulsetGenerationMismatch
        expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})
          description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesStatefulsetUpdateNotRolledOut
        expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})
          description: "StatefulSet update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesDaemonsetRolloutStuck
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})
          description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesDaemonsetMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})
          description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  ilimit.custom.certificates:
    groups:
    - name: ilimit.rules.certificates
      rules:

      - alert: KubernetesClientCertificateExpiresNextWeek
        expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes client certificate expires next week (instance {{ $labels.instance }})
          description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesClientCertificateExpiresSoon
        expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes client certificate expires soon (instance {{ $labels.instance }})
          description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

alertmanager:
  config:
    global:
       resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 1m
      repeat_interval: 5h
      receiver: dropped
      routes:
      - match:
          alertname: Watchdog
        receiver: dropped
        group_wait: 0s
        group_interval: 1m
        repeat_interval: 1m      
      - match:
          severity: critical
        continue: true
        receiver: dropped
      - match:
          severity: warning
        continue: true
        receiver: dropped
      - match:
          severity: 'info'
        continue: true
        receiver: dropped     
    receivers:
    - name: 'dropped'
    templates:
    - '/etc/alertmanager/config/*.tmpl'
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"
    labels: {}
    hosts:
      - kps-alermanager.ilba.cat
    path: /
  alertmanagerSpec: 
    storage:
    volumeClaimTemplate:
      spec:
        storageClassName: nfs-csi
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 666Gi


grafana:
  defaultDashboardsTimezone: Europe/Madrid
  adminPassword: sorisat2021
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"
    labels: {}
    hosts:
      - kps-grafana.ilba.cat
    path: /

prometheus:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"
    labels: {}
    hosts:
      - kps-prometheus.ilba.cat
    paths: 
      - /
  prometheusSpec:
    retention: 30d
    storageSpec:
     volumeClaimTemplate:
       spec:
         storageClassName: nfs-csi
         accessModes: ["ReadWriteOnce"]
         resources:
           requests:
             storage: 666Gi

kubeControllerManager:
  service:
    targetPort: 10257
  serviceMonitor:
    https: true
    insecureSkipVerify: true

